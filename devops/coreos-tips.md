# [Troubleshoot Common Issues](https://www.digitalocean.com/community/tutorials/how-to-troubleshoot-common-issues-with-your-coreos-servers)

1. Debugging your Cloud-Config File  (an invalid cloud-config file)

CoreOS requires that a cloud-config file be passed into your server upon creation. It uses the information contained within this file to bootstrap itself and initiate or join an existing cluster. It also starts essential services and can configure system basics such as users and groups.

Some things to check with your cloud-config file:

- Does it start with "#cloud-config"?: Every cloud-config file passed in must begin with "#cloud-config" standing alone on the first line. While this is usually an ignored comment in YAML, in this instance this line is used to signal to the cloud init system that this contains configuration data.

- Does your file contain valid YAML?: Cloud-config files are written in YAML, a data serialization format with a focus on readability. If you are having issues, paste your cloud-config into an online [YAML validator](http://codebeautify.org/yaml-validator). Your file should contain no errors. CoreOS provides a helpful tool that can check your cloud-config file's syntax, [Cloud-Config Validator](https://coreos.com/validate/).

- Are you Using a fresh discovery token? The discovery address keeps track of your machines' data even if the entire cluster is down. The discovery registration will fail when you boot up with an old token, especially if you had already registered the same IP address previously.

- Are you starting the fleet and etcd services?: Two services that must start in order for your cluster to function correctly are fleet and etcd.

You can only pass in the cloud-config file when the machine is created, so if you have made a mistake, destroy the server instance and start again (with a new token, in most cases).

Once you are fairly certain that the cloud-config file itself is correct, the next step is to log into the host to ensure that the file was processed correctly.

since no passwords are set on the CoreOS image by default, for security reasons. To work around this, you must recreate the server with a new cloud-config file which contains a password entry for the core user. You may want to add the password information to all of your cloud-config files as a general practice so you can troubleshoot. You can manually unset the password after verifying your connection.

The password must be in the form of a hash. You can generate these a few different ways depending on the software you have available. Any of the following will work, so use whichever option is best for you:

```bash
mkpasswd --method=SHA-512 --rounds=4096
openssl passwd -1
python -c "import crypt, getpass, pwd; print crypt.crypt('password', '\$6\$SALT\$')"
perl -e 'print crypt("password","\$6\$SALT\$") . "\n"'
```

Once you have a hash, you can add a new section to your cloud-config (outside of the "coreos" section), called users to place this information:

```bash
#cloud-config
users:
  - name: core
    passwd: hashed_password
coreos:
  . . .
```

2. Checking the Individual Host

- Check for Errors in the Essential Services

```bash
# If this returns without an error, it means that fleet and etcd were started correctly and that they are communicating with other hosts
fleetctl list-machines
```

Common error case:

```bash
2014/09/18 17:10:50 INFO client.go:278: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused
2014/09/18 17:10:50 ERROR client.go:200: Unable to get result \for {Get /_coreos.com/fleet/machines}, retrying \in 100ms
2014/09/18 17:10:50 INFO client.go:278: Failed getting response from http://127.0.0.1:4001/: dial tcp 127.0.0.1:4001: connection refused
2014/09/18 17:10:50 ERROR client.go:200: Unable to get result \for {Get /_coreos.com/fleet/machines}, retrying \in 200ms
```

Since this represents a stack of different components on top of each other, let's start at the top level and work down. Check the fleet service to see what errors it gives us:

```bash
# Check the fleet service to see what errors it gives us
systemctl status -l fleet

# each of our essential services, we should check the status and logs. The general way of doing this is
#  the state of the service and the last few log lines.
systemctl status -l $service
#  access to the full logs.
journalctl -b -u $service

# use the syntax from above.
systemctl status -l etcd
journalctl -b -u etcd
```

- Check the Filesystem to See the Configuration Files Generated by the Cloud-Config

to check is what service files were generated by the cloud-config.




# tutorials

1. Sending signals from one docker container to another

Simply make docker API calls using HTTP through its unix domain socket.

step 1, share /var/run/docker.sock from the host into the container

The socket we need is in `/var/run/docker.sock`, and we can share that when we launch the container with  docker run -v /var/run/docker.sock:/var/run/docker.sock ...`

Step 2, send HTTP through the socket. ex: use `netcat`

`echo -e "POST /containers/nginx/kill?signal=HUP HTTP/1.0\r\n" | nc -U /var/run/docker.sock`

Other example: `echo -e "GET /images/json HTTP/1.0\r\n" | nc -U /var/run/docker.sock`

2. Load balancing with coreos, confd and nginx

[project git repo](https://github.com/lordelph/confd-demo)
[project blog post](http://blog.dixo.net/2015/02/load-balancing-with-coreos/)

* a simple CoreOS cluster:   add new apache containers and have them automatically added to the load balancing backend

* a simple apache container:  our application – it will register itself with etcd allowing all available containers to be discovered

* a data-volume:  confd can write its configuration into

confd is a daemon which can be configured to watch for changes in etcd keys, and then generate configuration files from template files filled in with current etcd values. here is to is create a container to run confd which creates an nginx configuration load balancing our apache servers.

```bash
# confdata.service  provide us with a place to store our nginx config.
[Unit]
Description=Configuration Data Volume Service
After=docker.service
Requires=docker.service

[Service]
Type=oneshot   # expect ExecStart to run just once. systemd won't keep retrying the ExecStart
RemainAfterExit=yes  # allows the service appear successfully executed

ExecStartPre=-/usr/bin/docker rm conf-data  # clears the volume if it already exists,
ExecStart=/usr/bin/docker run -v /etc/nginx --name conf-data nginx echo "created new data container"  #  creates a new container
```

* a container running confd:   watches for changes in etcd and builds an ngnix configuration file to load balance amongst available containers

```bash
[Unit]
Description=Configuration Service

#our data volume must be ready
After=confdata.service
Requires=confdata.service

[Service]
EnvironmentFile=/etc/environment

#kill any existing confd
ExecStartPre=-/usr/bin/docker kill %n
ExecStartPre=-/usr/bin/docker rm %n

#we need to provide our confd container with the IP it can reach etcd
#on, the docker socket so it send HUP signals to nginx, and our data volume
# an environment variable COREOS_PRIVATE_IPV4 which is simply a copy of the variable from /etc/environment.
# in Dockerfile, this variable was used `COREOS_PRIVATE_IPV4`, must be provided at initalization
ExecStart=/usr/bin/docker run --rm \
  -e COREOS_PRIVATE_IPV4=${COREOS_PRIVATE_IPV4} \
  -v /var/run/docker.sock:/var/run/docker.sock \
  --volumes-from=conf-data \
  --name %n \
  username/demo-confd # user customized image

ExecStop=/usr/bin/docker stop -t 3 %n
Restart=on-failure

[X-Fleet]
#we need to be on the same machine as confdata.service
MachineOf=confdata.service   #scheduled on the same machine as the confdata.service
```
and test values `docker run --rm -ti --volumes-from=conf-data nginx grep -A6 'upstream backend' /etc/nginx/nginx.conf` existed

* an nginx container:    obtains its configuration from the shared volume

launch after the confd.service though, so that we’ve got a fresh configuration to use.

```bash
[Unit]
Description=Nginx Service
After=confd.service

#we won't want it to require the service - that would stop us restarting it, which is safe
#Requires=confd.service

[Service]
EnvironmentFile=/etc/environment
ExecStartPre=-/usr/bin/docker kill %n
ExecStartPre=-/usr/bin/docker rm %n
ExecStartPre=/usr/bin/docker pull nginx
ExecStart=/usr/bin/docker run --name %n -p 80:80 --volumes-from=conf-data nginx
ExecStop=/usr/bin/docker stop -t 3 %n
Restart=on-failure

[X-Fleet]
#we need to be on the same machine as confdata
MachineOf=confdata.service
```

3. Discovering etcd from inside a container in CoreOS

obtain the address of the `docker0` interface on the host, then get that into your container.

* Way 1, Pulling the etcd endpoint from inside the container (side effect)

Inside the container, you can use the address of the default gateway, as this will correspond with the docker0 interface on the host. uses a bit of grep and awk to build the endpoint,

```bash
#!/bin/bash
ETCD_ENDPOINT=$(route|grep default|awk '{print $2}'):4001
```

* Way 2, Pushing the etcd endpoint into the container (Recommend)

make the etcd service on the host write an environment file we can incorporate into our fleet units. include it into your cloud-config to create the file on newly minted or updated machines.

```bash
# create a new file.
write_files:
- path: /run/systemd/system/etcd.service.d/30-environment.conf
  permissions: 420
  content: |
    [Service]
    #write an environment file to use in other units
    ExecStartPost=/bin/bash -c "echo ETCD_ENDPOINT=${ETCD_ADDR} > /etc/etcd.environment"
```

or write by hand, then have it take effect with, but lose it the next time CoreOS updates itself

```bash
sudo systemctl daemon-reload
sudo systemctl restart etcd.service
```

You should see it created `/etc/etcd.environment` and we can include that in any fleet unit with `Environment=/etc/etcd`.environment, and from there it’s easy to use the ETCD_ENDPOINT variable to configure services.
